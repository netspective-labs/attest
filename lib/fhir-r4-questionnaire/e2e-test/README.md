# End-to-End Questionnaire to SQLite Test

This script demonstrates a full walk of FHIR R4 Questionnaires and their
associated LHC Form responses, turning them into governed SQL tables with
populated rows. It shows how to use the _questionnaire walker_ to discover
artifacts, generate transformation modules, normalize responses, and then
persist the results into a relational store.

## Quick Start

```bash
rm -f e2e-test.sqlite.db && ./e2e-test.sql.ts | sqlite3 e2e-test.sqlite.db
```

- Removes any existing SQLite database (`e2e-test.sqlite.db`)
- Runs the script, which emits SQL DDL/DML to STDOUT
- Pipes that SQL into SQLite, creating a fresh database with the schema and
  loaded data

### What happens

1. Discovery

   - Finds all `*.fhir-R4-questionnaire.json` and matching `*.lhc-form.json` in
     the test fixtures folder.
   - FHIR R4 Questionnaire JSONs in `../fixtures/questionnaires`
   - Matching LHC Form response JSONs in `../fixtures/responses`

2. Module generation and transformation

   - Generates TypeScript adapter modules for each questionnaire.
   - Uses those adapters to transform raw LHC Form responses into normalized
     JSON data transfer objects (DTOs).

3. SQL emission

   - Defines two governed tables:

     - `e2e_test_questionnaire` → questionnaires and their source text
     - `e2e_test_questionnaire_response` → responses and their transformed DTOs
   - Emits both the schema (DDL) and inserts (DML).

4. SQLite persistence

   - The emitted SQL is piped directly into SQLite, resulting in a fully
     populated database you can query immediately.

### What it demonstrates

This script is an end-to-end sanity test and working example that exercises the
questionnaire “walker” and proves we can turn discovered FHIR R4 Questionnaires
and their LHC Form responses into governed SQL DDL/DML suitable for persistence
and downstream analytics.

This end-to-end test proves that the artifacts generated by the walker are not
just discoverable, but can also be transformed and captured in a governed SQL
schema — making them immediately usable for analysis, auditing, or downstream
applications.

## Technical Elaboration

1. Permissions and setup the script runs with read/write/run/env permissions and
   resolves paths relative to the current working directory. It creates a temp
   work directory for generated artifacts.

2. Discovery and transformation (Walker)

   - Constructs a `Walker` with:
     - `questionnairesHome` → directory of `*.fhir-R4-questionnaire.json`
     - `respHome` → directory of `*.lhc-form.json`

   - `await walker.walk("leave-work-dir")`:
     - Discovers questionnaires and their responses
     - Generates TypeScript modules (`*.auto.ts`) for each questionnaire
     - Dynamically imports those modules, extracts their module signatures, and
       runs the LHC Form response adapter function
     - Attaches the transformed response JSON back onto each response

3. Governed schema definition (sql-aide)

   - Uses `sql-aide` and the “typical” pattern helpers to create a governed
     model with consistent keys and housekeeping columns.
   - Creates two 3NF tables:
     - `e2e_test_questionnaire`
       - Tracks a questionnaire’s identity, source filename, original
         questionnaire JSON text (if `--include-src` was used), and a
         placeholder for bundled JS (future).
     - `e2e_test_questionnaire_response`
       - Tracks each response, a foreign key to its questionnaire (via title
         mapping), the raw LHC response JSON, and the transformed DTO JSON.
   - ULIDs are generated with `monotonicUlid()` for stable, sortable
     identifiers.

4. DML assembly

   - Iterates discovered modules and responses:
     - Inserts a row per questionnaire into `e2e_test_questionnaire`
     - Inserts a row per response into `e2e_test_questionnaire_response`,
       including both original and transformed JSON payloads
   - The questionnaire title is used as a lookup key to relate responses to
     their questionnaire row.

5. Emission and cleanup

   - Composes a single SQL script containing:
     - Governance comments
     - Table DDL
     - All INSERT DML
   - Prints SQL to STDOUT for capture and piping to SQLite
   - Cleans up the temp work directory

## What you can learn from e2e-test

- A complete “file system → codegen → reflection → transformation → SQL”
  pipeline that is reproducible and automatable.
- Type-safe response transformation using generated adapters rather than ad-hoc
  parsing.
- A small, governed relational footprint that preserves provenance:
  - Raw questionnaire source text (if available)
  - Raw LHC response JSON
  - Transformed JSON DTO
- Practical use of ULIDs for monotonic, globally unique keys appropriate for
  batch ingestion.
- “Capturable executable” pattern: the entire result is streamable as SQL for
  ingestion by surveilr or any SQL-capable system.

## Outputs

- One Deno TypeScript executable with generates SQL DDL and DML to STDOUT that:
  - Creates the `e2e_test_questionnaire` and `e2e_test_questionnaire_response`
    tables when piped to SQLite
  - Inserts a row for each discovered questionnaire and response, including
    embedded JSON payloads when piped to SQLite

## Notes and extensibility

- `module_js` is intentionally a placeholder; future steps can `deno bundle` the
  generated modules into a single JavaScript artifact per questionnaire.
- The schema is intentionally minimal; add views or additional tables for richer
  analytics without denormalizing the core tables.
- The walker’s module signature check ensures adapters are present and helps
  guard against drift between questionnaires and transformation code.
